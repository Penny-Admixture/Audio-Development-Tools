# Audio-Development-Tools (ADT)

This is a list of sound, audio and music development tools which contains machine learning, audio generation, audio signal processing, sound synthesis, spatial audio, music information retrieval, music generation, speech recognition, speech synthesis, singing voice synthesis and more.

The repository also includes `comfyui_workflow_ui.py`, a small utility that
generates a Tkinter user interface for the adjustable parameters in a ComfyUI
workflow JSON or PNG file.

* [Machine Learning (ML)](#ml)
* [Audio Generation (AG)](#ag)
* [Audio Signal Processing (ASP)](#asp)
* [Sound Synthesis (SS)](#ss)
* [Spatial Audio (SA)](#sa)
* [Web Audio Processing (WAP)](#wap)
* [Music Information Retrieval (MIR)](#mir)
* [Music Generation (MG)](#mg)
* [Speech Recognition (ASR)](#asr)
* [Speech Synthesis (TTS)](#tts)
* [Singing Voice Synthesis (SVS)](#svs)

## <span id="ml">Machine Learning (ML)</span>

* [librosa](https://librosa.org/doc/latest/index.html) - Librosa is a python package for music and audio analysis. It provides the building blocks necessary to create music information retrieval systems.
* [Essentia](http://essentia.upf.edu/) - Essentia is an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPLv3 license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. C++ library for audio and music analysis, description and synthesis, including Python bindings.
* [DDSP](https://github.com/magenta/ddsp) - DDSP: Differentiable Digital Signal Processing. DDSP is a library of differentiable versions of common DSP functions (such as synthesizers, waveshapers, and filters). This allows these interpretable elements to be used as part of an deep learning model, especially as the output layers for audio generation.
* [MIDI-DDSP](https://github.com/magenta/midi-ddsp) - MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling. MIDI-DDSP is a hierarchical audio generation model for synthesizing MIDI expanded from DDSP.
* [torchsynth](https://github.com/torchsynth/torchsynth) - A GPU-optional modular synthesizer in pytorch, 16200x faster than realtime, for audio ML researchers.
* [aubio](https://aubio.org/) - aubio is a tool designed for the extraction of annotations from audio signals. Its features include segmenting a sound file before each of its attacks, performing pitch detection, tapping the beat and producing midi streams from live audio.
* [audioFlux](https://audioflux.top/) - audioFlux is a deep learning tool library for audio and music analysis, feature extraction. It supports dozens of time-frequency analysis transformation methods and hundreds of corresponding time-domain and frequency-domain feature combinations. It can be provided to deep learning networks for training, and is used to study various tasks in the audio field such as Classification, Separation, Music Information Retrieval(MIR) and ASR etc.
* [Polymath](https://github.com/samim23/polymath) - Polymath uses machine learning to convert any music library (e.g from Hard-Drive or YouTube) into a music production sample-library. The tool automatically separates songs into stems (beats, bass, etc.), quantizes them to the same tempo and beat-grid (e.g. 120bpm), analyzes musical structure (e.g. verse, chorus, etc.), key (e.g C4, E3, etc.) and other infos (timbre, loudness, etc.), and converts audio to midi. The result is a searchable sample library that streamlines the workflow for music producers, DJs, and ML audio developers.
* [IPython](https://ipython.readthedocs.io/en/stable/index.html) - IPython provides a rich toolkit to help you make the most of using Python interactively.
* [torchaudio](https://github.com/pytorch/audio) - an audio library for PyTorch. Data manipulation and transformation for audio signal processing, powered by PyTorch.
* [torch-audiomentations](https://github.com/asteroid-team/torch-audiomentations) - Fast audio data augmentation in PyTorch. Inspired by audiomentations. Useful for deep learning.
* [PyTorch Audio Augmentations](https://github.com/Spijkervet/torchaudio-augmentations) - Audio data augmentations library for PyTorch for audio in the time-domain. 
* [Asteroid](https://asteroid-team.github.io/) - Asteroid is a Pytorch-based audio source separation toolkit that enables fast experimentation on common datasets. It comes with a source code that supports a large range of datasets and architectures, and a set of recipes to reproduce some important papers.
* [Kapre](https://kapre.readthedocs.io/en/latest/#) - Kapre: Keras Audio Preprocessors. Keras Audio Preprocessors - compute STFT, InverseSTFT, Melspectrogram, and others on GPU real-time.
* [praudio](https://github.com/musikalkemist/praudio) - Audio preprocessing framework for Deep Learning audio applications.
* [DeepAFx](https://github.com/adobe-research/DeepAFx) - DeepAFx: Deep Audio Effects. Audio signal processing effects (FX) are used to manipulate sound characteristics across a variety of media. Many FX, however, can be difficult or tedious to use, particularly for novice users. In our work, we aim to simplify how audio FX are used by training a machine to use FX directly and perform automatic audio production tasks. By using familiar and existing tools for processing and suggesting control parameters, we can create a unique paradigm that blends the power of AI with human creative control to empower creators.
* [nnAudio](https://github.com/KinWaiCheuk/nnAudio) - nnAudio is an audio processing toolbox using PyTorch convolutional neural network as its backend. By doing so, spectrograms can be generated from audio on-the-fly during neural network training and the Fourier kernels (e.g. or CQT kernels) can be trained.
* [WavEncoder](https://github.com/shangeth/wavencoder) - WavEncoder is a Python library for encoding audio signals, transforms for audio augmentation, and training audio classification models with PyTorch backend.
* [SciPy](https://scipy.org/) - SciPy (pronounced "Sigh Pie") is an open-source software for mathematics, science, and engineering. It includes modules for statistics, optimization, integration, linear algebra, Fourier transforms, signal and image processing, ODE solvers, and more.
* [pyAudioAnalysis](https://github.com/tyiannak/pyAudioAnalysis/) - Python Audio Analysis Library: Feature Extraction, Classification, Segmentation and Applications.
* [Mutagen](https://mutagen.readthedocs.io/en/latest/#) - Mutagen is a Python module to handle audio metadata. It supports ASF, FLAC, MP4, Monkey‚Äôs Audio, MP3, Musepack, Ogg Opus, Ogg FLAC, Ogg Speex, Ogg Theora, Ogg Vorbis, True Audio, WavPack, OptimFROG, and AIFF audio files. All versions of ID3v2 are supported, and all standard ID3v2.4 frames are parsed. It can read Xing headers to accurately calculate the bitrate and length of MP3s. ID3 and APEv2 tags can be edited regardless of audio format. It can also manipulate Ogg streams on an individual packet/page level.
* [LibXtract](https://github.com/jamiebullock/LibXtract) - LibXtract is a simple, portable, lightweight library of audio feature extraction functions. The purpose of the library is to provide a relatively exhaustive set of feature extraction primatives that are designed to be 'cascaded' to create a extraction hierarchies.
* [dejavu](https://github.com/worldveil/dejavu) - Audio fingerprinting and recognition in Python. Dejavu can memorize audio by listening to it once and fingerprinting it. Then by playing a song and recording microphone input or reading from disk, Dejavu attempts to match the audio against the fingerprints held in the database, returning the song being played.
* [Matchering](https://github.com/sergree/matchering) - üéöÔ∏è Open Source Audio Matching and Mastering. **[Matchering 2.0](https://github.com/sergree/matchering)** is a novel **[Containerized Web Application](https://github.com/sergree/matchering#docker-image---the-easiest-way)** and **[Python Library](https://pypi.org/project/matchering)** for audio matching and [mastering](https://en.wikipedia.org/wiki/Audio_mastering).
* [TimeSide](https://github.com/Parisson/TimeSide) - TimeSide is a python framework enabling low and high level audio analysis, imaging, transcoding, streaming and labelling. Its high-level API is designed to enable complex processing on very large datasets of any audio or video assets with a plug-in architecture, a secure scalable backend and an extensible dynamic web frontend.
* [Meyda](https://meyda.js.org/) - Meyda is a Javascript audio feature extraction library. Meyda supports both offline feature extraction as well as real-time feature extraction using the [Web Audio API](https://github.com/WebAudio/web-audio-api). We wrote a paper about it, which is available [here](https://wac.ircam.fr/pdf/wac15_submission_17.pdf).
* [Audiomentations](https://github.com/iver56/audiomentations) - A Python library for audio data augmentation. Inspired by albumentations. Useful for deep learning. Runs on CPU. Supports mono audio and multichannel audio. Can be integrated in training pipelines in e.g. Tensorflow/Keras or Pytorch. Has helped people get world-class results in Kaggle competitions. Is used by companies making next-generation audio products.
* [soundata](https://github.com/soundata/soundata) - Python library for downloading, loading & working with sound datasets.

## <span id="ag">Audio Generation (AG)</span>

* [Bark](https://github.com/suno-ai/bark) - Bark is a transformer-based text-to-audio model created by Suno. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise and simple sound effects. 
* [ArchiSound](https://github.com/archinetai/audio-diffusion-pytorch) - Audio generation using diffusion models, in PyTorch.
* [NeuralSound](https://github.com/hellojxt/NeuralSound) - Learning-based Modal Sound Synthesis with Acoustic Transfer.
* [RAVE](https://github.com/acids-ircam/RAVE) - RAVE: Realtime Audio Variational autoEncoder. A variational autoencoder for fast and high-quality neural audio synthesis.
* [AudioLDM](https://audioldm.github.io/) - AudioLDM: Text-to-Audio Generation with Latent Diffusion Models.
* [Make-An-Audio](https://text-to-audio.github.io/) - Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models.
* [Mo√ªsai](https://anonymous0.notion.site/anonymous0/Mo-sai-Text-to-Audio-with-Long-Context-Latent-Diffusion-b43dbc71caf94b5898f9e8de714ab5dc) - Mo√ªsai: Text-to-Audio with Long-Context Latent Diffusion.
* [Im2Wav](https://github.com/RoySheffer/im2wav) - Image Guided Audio Generation. We propose Im2Wav, an image guided open-domain audio generation system. Given an input image or a sequence of images, Im2Wav generates a semantically relevant sound.

## <span id="asp">Audio Signal Processing (ASP)</span>

* [SouPyX](https://github.com/Yuan-ManX/SouPyX) - SouPyX is a very colourful space for audio exploration, suitable for research and exploration in a variety of audio fields. In SouPyX you can carry out research and exploration in audio processing, sound synthesis, audio effects, spatial audio, audio visualisation, AI audio and much more.
* [SoundFile](https://pysoundfile.readthedocs.io/en/latest/) - SoundFile is an audio library based on libsndfile, CFFI and NumPy.
* [Audio DSPy](https://github.com/jatinchowdhury18/audio_dspy) - audio_dspy is a Python package for audio signal processing tools.
* [pyAudioDspTools](https://pyaudiodsptools.readthedocs.io/en/latest/#) - pyAudioDspTools is a python 3 package for manipulating audio by just using numpy.
* [wave](https://docs.python.org/3/library/wave.html) - The wave module provides a convenient interface to the WAV sound format. It does not support compression/decompression, but it does support mono/stereo.
* [Pedalboard](https://github.com/spotify/pedalboard) - Pedalboard is a Python library for working with audio: reading, writing, adding effects, and more. It supports most popular audio file formats and a number of common audio effects out of the box, and also allows the use of VST3 and Audio Unit formats for third-party plugins.
* [PyAudio](https://people.csail.mit.edu/hubert/pyaudio/) - PyAudio provides [Python](http://www.python.org/) bindings for [PortAudio](http://www.portaudio.com/) v19, the cross-platform audio I/O library. With PyAudio, you can easily use Python to play and record audio on a variety of platforms, such as GNU/Linux, Microsoft Windows, and Apple macOS.
* [PortAudio](http://www.portaudio.com/) - PortAudio is a free, cross-platform, [open-source](http://www.portaudio.com/license.html), audio I/O library.  It lets you write simple audio programs in 'C' or C++ that will compile and run on many platforms including Windows, Macintosh OS X, and Unix (OSS/ALSA). It is intended to promote the exchange of audio software between developers on different platforms. Many [applications](http://www.portaudio.com/apps.html) use PortAudio for Audio I/O.
* [Pyo](https://github.com/belangeo/pyo) - pyo is a Python module written in C to help digital signal processing script creation.Python DSP module. With pyo, user will be able to include signal processing chains directly in Python scripts or projects, and to manipulate them in real time through the interpreter
* [tinytag](https://github.com/devsnd/tinytag) - tinytag is a library for reading music meta data of most common audio files in pure python. Read audio and music meta data and duration of MP3, OGG, OPUS, MP4, M4A, FLAC, WMA, Wave and AIFF files with python 2 or 3.
* [Friture](https://friture.org/) - **Friture** is an application to visualize and analyze live audio data in real-time. Friture displays audio data in several widgets, such as a scope, a spectrum analyzer, or a rolling 2D spectrogram.
* [sounddevice](https://pypi.org/project/sounddevice/) - This [Python](https://www.python.org/) module provides bindings for the [PortAudio](http://www.portaudio.com/) library and a few convenience functions to play and record [NumPy](https://numpy.org/) arrays containing audio signals.
* [Pydub](https://github.com/jiaaro/pydub) - Manipulate audio with a simple and easy high level interface.
* [SoundCard](https://soundcard.readthedocs.io/en/latest/) - SoundCard is a library for playing and recording audio without resorting to a CPython extension. Instead, it is implemented using the wonderful [CFFI](http://cffi.readthedocs.io/en/latest/) and the native audio libraries of Linux, Windows and macOS.
* [TarsosDSP](https://github.com/JorenSix/TarsosDSP) - TarsosDSP is a Java library for audio processing. Its aim is to provide an easy-to-use interface to practical music processing algorithms implemented, as simply as possible, in pure Java and without any other external dependencies.
* [JUCE](https://github.com/juce-framework/JUCE) - JUCE is an open-source cross-platform C++ application framework for creating high quality desktop and mobile applications, including VST, VST3, AU, AUv3, AAX and LV2 audio plug-ins and plug-in hosts. JUCE can be easily integrated with existing projects via CMake, or can be used as a project generation tool via the [Projucer](https://juce.com/discover/projucer), which supports exporting projects for Xcode (macOS and iOS), Visual Studio, Android Studio, Code::Blocks and Linux Makefiles as well as containing a source code editor.
* [Q](https://cycfi.github.io/q/) - Q is a cross-platform C++ library for Audio Digital Signal Processing. Aptly named after the ‚ÄúQ factor‚Äù, a dimensionless parameter that describes the quality of a resonant circuit, the Q DSP Library is designed to be simple and elegant, as the simplicity of its name suggests, and efficient enough to run on small microcontrollers.
* [BasicDSP](https://github.com/trcwm/BasicDSP) - BasicDSP - A tool for processing audio / experimenting with signal processing.
* [DaisySP](https://github.com/electro-smith/DaisySP) - A Powerful, Open Source DSP Library in C++.
* [Speech Signal Processing Toolkit (SPTK)](http://sp-tk.sourceforge.net/) - The Speech Signal Processing Toolkit (SPTK) is a suite of speech signal processing tools for UNIX environments, e.g., LPC analysis, PARCOR analysis, LSP analysis, PARCOR synthesis filter, LSP synthesis filter, vector quantization techniques, and other extended versions of them.
* [eDSP](https://mohabouje.github.io/edsp-docs/) - *eDSP* (easy Digital Signal Processing) is a digital signal processing framework written in modern C++ that implements some of the common functions and algorithms frequently used in digital signal processing, audio engineering & telecommunications systems.
* [KFR](https://www.kfrlib.com/) - KFR is an open source C++ DSP framework that focuses on high performance. Fast, modern C++ DSP framework, FFT, Sample Rate Conversion, FIR/IIR/Biquad Filters (SSE, AVX, AVX-512, ARM NEON).
* [MWEngine](https://github.com/igorski/MWEngine) - Audio engine and DSP for Android, written in C++ providing low latency performance within a musical context, while providing a Java/Kotlin API. Supports both OpenSL and AAudio.
* [LabSound](https://github.com/LabSound/LabSound) - LabSound is a C++ graph-based audio engine. The engine is packaged as a batteries-included static library meant for integration in many types of software: games, visualizers, interactive installations, live coding environments, VST plugins, audio editing/sequencing applications, and more.
* [Gist](https://github.com/adamstark/Gist) - Gist is a C++ based audio analysis library.
* [Realtime_PyAudio_FFT](https://github.com/aiXander/Realtime_PyAudio_FFT) - Realtime audio analysis in Python, using PyAudio and Numpy to extract and visualize FFT features from streaming audio.
* [Spectrum](https://github.com/cokelaer/spectrum) - Spectral Analysis in Python. Spectrum is a Python library that contains tools to estimate Power Spectral Densities based on Fourier transform, Parametric methods or eigenvalues analysis. The Fourier methods are based upon correlogram, periodogram and Welch estimates. Standard tapering windows (Hann, Hamming, Blackman) and more exotic ones are available (DPSS, Taylor, ‚Ä¶).

## <span id="ss">Sound Synthesis (SS)</span>

* [Csound](https://csound.com/) - Csound is a sound and music computing system which was originally developed by Barry Vercoe in 1985 at MIT Media Lab. Since the 90s, it has been developed by a group of core developers.
* [Pure Data](https://puredata.info/) - **Pure Data** ( **Pd** ) is a [visual programming language](https://en.wikipedia.org/wiki/Visual_programming_language "Visual programming language") developed by [Miller Puckette](https://en.wikipedia.org/wiki/Miller_Puckette "Miller Puckette") in the 1990s for creating [interactive](https://en.wikipedia.org/wiki/Interaction "Interaction") [computer music](https://en.wikipedia.org/wiki/Computer_music "Computer music") and [multimedia](https://en.wikipedia.org/wiki/Multimedia "Multimedia") works. While Puckette is the main author of the program, Pd is an [open-source](https://en.wikipedia.org/wiki/Open-source_software "Open-source software") project with a large developer base working on new extensions. It is released under [BSD-3-Clause](https://en.wikipedia.org/wiki/BSD_licenses "BSD licenses"). It runs on [Linux](https://en.wikipedia.org/wiki/Linux "Linux"), [MacOS](https://en.wikipedia.org/wiki/MacOS "MacOS"), [iOS](https://en.wikipedia.org/wiki/IOS "IOS"), [Android](https://en.wikipedia.org/wiki/Android_(operating_system)) "Android (operating system)") and [Windows](https://en.wikipedia.org/wiki/Windows "Windows"). Ports exist for [FreeBSD](https://en.wikipedia.org/wiki/FreeBSD "FreeBSD") and [IRIX](https://en.wikipedia.org/wiki/IRIX "IRIX").
* [plugdata](https://plugdata.org/) - A visual programming environment for audio experimentation, prototyping and education.
* [Max/MSP/Jitter](https://cycling74.com/) - **Max** , also known as Max/MSP/Jitter, is a [visual programming language](https://en.wikipedia.org/wiki/Visual_programming_language "Visual programming language") for [music](https://en.wikipedia.org/wiki/Music "Music") and [multimedia](https://en.wikipedia.org/wiki/Multimedia "Multimedia") developed and maintained by [San Francisco](https://en.wikipedia.org/wiki/San_Francisco "San Francisco")-based software company [Cycling &#39;74](https://en.wikipedia.org/wiki/Cycling_%2774 "Cycling '74"). Over its more than thirty-year history, it has been used by composers, performers, software designers, researchers, and artists to create recordings, performances, and installations.
* [Kyma (sound design language)](https://kyma.symbolicsound.com/) - **Kyma** is a visual programming language for sound design used by musicians, researchers, and sound designers. In Kyma, a user programs a multiprocessor DSP by graphically connecting modules on the screen of a [Macintosh](https://en.wikipedia.org/wiki/Macintosh "Macintosh") or [Windows](https://en.wikipedia.org/wiki/Microsoft_Windows "Microsoft Windows") computer.
* [SuperCollider](https://supercollider.github.io/) - **SuperCollider** is a platform for audio synthesis and algorithmic composition, used by musicians, artists, and researchers working with sound. An audio server, programming language, and IDE for sound synthesis and algorithmic composition.
* [Sonic Pi](https://sonic-pi.net/) - **Sonic Pi** is a [live coding](https://en.wikipedia.org/wiki/Live_coding "Live coding") environment based on [Ruby](https://en.wikipedia.org/wiki/Ruby_(programming_language)) "Ruby (programming language)"), originally designed to support both computing and music lessons in schools, developed by Sam Aaron in the [University of Cambridge Computer Laboratory](https://en.wikipedia.org/wiki/Computer_Laboratory,_University_of_Cambridge "Computer Laboratory, University of Cambridge") in collaboration with [Raspberry Pi Foundation](https://en.wikipedia.org/wiki/Raspberry_Pi_Foundation "Raspberry Pi Foundation").
* [Reaktor](https://en.wikipedia.org/wiki/Reaktor) - **Reaktor** is a graphical [modular software music studio](https://en.wikipedia.org/wiki/Modular_software_music_studio "Modular software music studio") developed by [Native Instruments](https://en.wikipedia.org/wiki/Native_Instruments "Native Instruments") (NI). It allows musicians and sound specialists to design and build their own instruments, [samplers](https://en.wikipedia.org/wiki/Sampler_(musical_instrument)) "Sampler (musical instrument)"), effects and sound design tools. It is supplied with many ready-to-use instruments and effects, from emulations of classic [synthesizers](https://en.wikipedia.org/wiki/Synthesizer "Synthesizer") to futuristic sound design tools.
* [RTcmix](http://rtcmix.org/) - **RTcmix** is a real-time software "language" for doing digital sound synthesis and signal-processing. It is written in C/C++, and is distributed open-source, free of charge.
* [ChucK](https://chuck.stanford.edu/) - ChucK is a programming language for real-time sound synthesis and music creation. ChucK offers a unique time-based, concurrent programming model that is precise and expressive (we call this strongly-timed), dynamic control rates, and the ability to add and modify code on-the-fly. In addition, ChucK supports MIDI, OpenSoundControl, HID device, and multi-channel audio. It is open-source and freely available on MacOS X, Windows, and Linux. It's fun and easy to learn, and offers composers, researchers, and performers a powerful programming tool for building and experimenting with complex audio synthesis/analysis programs, and real-time interactive music.
* [Faust](https://faust.grame.fr/) - Faust (Functional Audio Stream) is a functional programming language for sound synthesis and audio processing with a strong focus on the design of synthesizers, musical instruments, audio effects, etc. Faust targets high-performance signal processing applications and audio plug-ins for a variety of platforms and standards.
* [SOUL](https://soul.dev/) - The SOUL programming language and API. SOUL (SOUnd Language) is an attempt to modernise and optimise the way high-performance, low-latency audio code is written and executed.
* [Cmajor](https://cmajor.dev/) - Cmajor is a programming language for writing fast, portable audio software. You've heard of C, C++, C#, objective-C... well, C*major* is a C-family language designed specifically for writing DSP signal processing code.
* [Gwion](https://github.com/Gwion/Gwion) - Gwion is a programming language, aimed at making music. **strongly** inspired by [ChucK](http://chuck.stanford.edu/), but adding a bunch *high-level* features; templating, first-class functions and more. It aims to be simple, small, [fast](https://gwion.github.io/Gwion/#Benchmarks/), [extendable](https://github.com/Gwion/Gwion-plug) and [embeddable](https://github.com/Gwion/Gwion/blob/master/src/main.c#L18-L31).
* [Elementary Audio](https://www.elementary.audio/) - Elementary is a JavaScript framework and high performance audio engine that helps you build quickly and ship confidently. Declarative, functional framework for writing audio software on the web or for native apps.
* [Sound2Synth](https://github.com/Sound2Synth/Sound2Synth) - Sound2Synth: Interpreting Sound via FM Synthesizer Parameters Estimation.
* [JSyn](http://www.softsynth.com/jsyn/) - JSyn is a modular audio synthesizer for Java by Phil Burk. JSyn allows you to [develop](http://www.softsynth.com/jsyn/developers/) interactive computer music programs in Java. It can be used to generate sound effects, audio environments, or music. JSyn is based on the traditional model of unit generators which can be connected together to form complex sounds.
* [Midica](http://www.midica.org/) - Midica is an interpreter for a Music Programming Language. It translates source code to MIDI. But it can also be used as a MIDI Player, MIDI compiler or decompiler, Karaoke Player, ALDA Player, ABC Player, LilyPond Player or a MIDI File Analyzer. You write music with one of the supported languages (MidicaPL, ALDA or ABC).
* [Mercury](https://www.timohoogland.com/mercury-livecoding/) - Mercury is a minimal and human-readable language for the live coding of algorithmic electronic music. All elements of the language are designed around making code more accessible and less obfuscating for the audience. This motivation stretches down to the coding style itself which uses clear descriptive names for functions and a clear syntax.
* [Alda](https://alda.io/) - Alda is a text-based programming language for music composition. It allows you to write and play back music using only a text editor and the command line. The language‚Äôs design equally favors aesthetics, flexibility and ease of use.
* [Platonic Music Engine](https://www.platonicmusicengine.com/) - The *Platonic Music Engine* is an attempt to create computer algorithms that superficially simulate the entirety of creative human culture, past, present, and future. It does so in an interactive manner allowing the user to choose various parameters and settings such that the final result will be unique to the user while still preserving the cultural idea that inspired the work.
* [pyo-tools](https://github.com/belangeo/pyo-tools) - Repository of ready-to-use python classes for building audio effects and synths with pyo.
* [py-modular](https://py-modular.readthedocs.io/en/latest/) - Modular and experimental audio programming framework for Python. py-modular is a small, experimental audio programming environment for python. It is intended to be a base for exploration of new audio technologies and workflows. Most everything in py-modular is built around a node-based workflow, meaning small classes do small tasks and can be patched together to create full synthesizers or larger ideas.
* [Bach: Automated Composer&#39;s Helper ](https://www.bachproject.net/) - a cross-platform set of patches and externals for Max, aimed to bring the richness of computer-aided composition into the real-time world.
* [AudioKit](https://github.com/AudioKit/AudioKit) - AudioKit is an audio synthesis, processing, and analysis platform for iOS, macOS (including Catalyst), and tvOS.
* [Twang](https://github.com/AldaronLau/twang) - Library for pure Rust advanced audio synthesis.
* [Gensound](https://github.com/Quefumas/gensound) - Pythonic audio processing and generation framework. The Python way to audio processing & synthesis.
* [OTTO](https://bitfieldaudio.com/) - The OTTO is a digital hardware groovebox, with synths, samplers, effects and a sequencer with an audio looper. The interface is flat, modular and easy to use, but most of all, it aims to encourage experimentation.
* [Loris](https://github.com/tractal/loris) - Loris is a library for sound analysis, synthesis, and morphing, developed by Kelly Fitz and Lippold Haken at the CERL Sound Group. Loris includes a C++ class library, Python module, C-linkable interface, command line utilities, and documentation.
* [IanniX](https://www.iannix.org/fr/) - IanniX is a graphical open-source sequencer, based on Iannis Xenakis works, for digital art. IanniX syncs via Open Sound Control (OSC) events and curves to your real-time environment.
* [Leipzig](https://github.com/ctford/leipzig) - A music composition library for Clojure and Clojurescript.
* [Nyquist](https://www.cs.cmu.edu/~music/nyquist/) - Nyquist is a sound synthesis and composition language offering a Lisp syntax as well as an imperative language syntax and a powerful integrated development environment.. Nyquist is an elegant and powerful system based on functional programming.
* [OpenMusic (OM)](http://repmus.ircam.fr/openmusic/home) - OpenMusic (OM) is a visual programming language based on [Lisp](http://www.gigamonkeys.com/book/introduction-why-lisp.html "http://www.gigamonkeys.com/book/introduction-why-lisp.html"). Visual programs are created by assembling and connecting icons representing functions and data structures. Most programming and operations are performed by dragging an icon from a particular place and dropping it to an other place. Built-in visual control structures (e.g. loops) are provided, that interface with Lisp ones. Existing CommonLisp/CLOS code can easily be used in OM, and new code can be developed in a visual way.
* [ORCŒõ](https://github.com/hundredrabbits/Orca) - Orca is an [esoteric programming language](https://en.wikipedia.org/wiki/Esoteric_programming_language) designed to quickly create procedural sequencers, in which every letter of the alphabet is an operation, where lowercase letters operate on bang, uppercase letters operate each frame.
* [Overtone](http://overtone.github.io/) - Overtone is an open source audio environment designed to explore new musical ideas from synthesis and sampling to instrument building, live-coding and collaborative jamming. We combine the powerful [SuperCollider](http://supercollider.github.io/) audio engine, with [Clojure](http://clojure.org/), a state of-the-art lisp, to create an intoxicating interactive sonic experience.
* [SEAM](https://grammaton.gitbook.io/seam/) - Sustained Electro-Acoustic Music - Base. *Sustained Electro-Acoustic Music* is a project inspired by [Alvise Vidolin and Nicola Bernardini](https://www.academia.edu/16348988/Sustainable_live_electro-acoustic_music).
* [Glicol](https://glicol.org/) - Glicol (an acronym for "graph-oriented live coding language") is a computer music language with both its language and audio engine written in [Rust programming language](https://www.rust-lang.org/), a modern alternative to C/C++. Given this low-level nature, Glicol can run on many different platforms such as browsers, VST plugins and Bela board. Glicol's synth-like syntax and powerful audio engine also make it possible to combine high-level synth or sequencer control with low-level sample-accurate audio synthesis, all in real-time.
* [PaperSynth](https://github.com/Ashvala/PaperSynth) - Handwritten text to synths! PaperSynth is a project that aims to read keywords you've written on a piece of paper and convert it into synthesizers you can play on the phone.
* [Neural Resonator VST](https://github.com/rodrigodzf/NeuralResonatorVST) - This is a VST plugin that uses a neural network to generate filters based on arbitrary 2D shapes and materials. It is possible to use midi to trigger simple impulses to excite these filters. Additionally any audio signal can be used as input to the filters.
* [Scyclone](https://github.com/Torsion-Audio/Scyclone) - Scyclone is an audio plugin that utilizes neural timbre transfer technology to offer a new approach to audio production. The plugin builds upon RAVE methodology, a realtime audio variational auto encoder, facilitating neural timbre transfer in both single and couple inference mode.
* [mlinmax](https://github.com/estine/mlinmax) - ML for sound generation and processing in Cycling '74's Max programming language.
* [ADLplug](https://github.com/jpcima/ADLplug) - FM Chip Synthesizer ‚Äî OPL & OPN ‚Äî VST/LV2/Standalone.
* [Surge](https://github.com/surge-synthesizer/surge) - Synthesizer plug-in (previously released as Vember Audio Surge). 
* [cStop](https://github.com/calgoheen/cStop) - cStop is a tape stop audio effect plugin available in AU & VST3 for Mac (Windows coming soon).

## <span id="sa">Spatial Audio (SA)</span>

* [spaudiopy](https://spaudiopy.readthedocs.io/en/latest/index.html) - Spatial Audio Python Package. The focus (so far) is on spatial audio encoders and decoders. The package includes e.g. spherical harmonics processing and (binaural renderings of) loudspeaker decoders, such as VBAP and AllRAD.
* [Spatial_Audio_Framework (SAF)](https://leomccormack.github.io/Spatial_Audio_Framework/) - The Spatial_Audio_Framework (SAF) is an open-source and cross-platform framework for developing spatial audio related algorithms and software in C/C++. Originally intended as a resource for researchers in the field, the framework has gradually grown into a rather large and well-documented codebase comprising a number of distinct [**modules**](https://github.com/leomccormack/Spatial_Audio_Framework/blob/master/framework/modules); with each module targeting a specific sub-field of spatial audio (e.g. Ambisonics encoding/decoding, spherical array processing, amplitude-panning, HRIR processing, room simulation, etc.).
* [HO-SIRR](https://github.com/leomccormack/HO-SIRR) - Higher-order Spatial Impulse Response Rendering (HO-SIRR) is a rendering method, which can synthesise output loudspeaker array room impulse responses (RIRs) using input spherical harmonic (Ambisonic/B-Format) RIRs of arbitrary order. A MATLAB implementation of the Higher-order Spatial Impulse Response Rendering (HO-SIRR) algorithm; an alternative approach for reproducing Ambisonic RIRs over loudspeakers.
* [SpatGRIS](https://github.com/GRIS-UdeM/SpatGRIS) - SpatGRIS is a sound spatialization software that frees composers and sound designers from the constraints of real-world speaker setups. With the ControlGRIS plugin distributed with SpatGRIS, rich spatial trajectories can be composed directly in your DAW and reproduced in real-time on any speaker layout. It is fast, stable, cross-platform, easy to learn and works with the tools you already know. SpatGRIS supports any speaker setup, including 2D layouts like quad, 5.1 or octophonic rings, and 3D layouts like speaker domes, concert halls, theatres, etc. Projects can also be mixed down to stereo using a binaural head-related transfer function or simple stereo panning.
* [Steam Audio](https://github.com/ValveSoftware/steam-audio) - Steam Audio delivers a full-featured audio solution that integrates environment and listener simulation. HRTF significantly improves immersion in VR; physics-based sound propagation completes aural immersion by consistently recreating how sound interacts with the virtual environment.
* [libmysofa](https://github.com/hoene/libmysofa) - Reader for AES SOFA files to get better HRTFs.
* [Omnitone](https://googlechrome.github.io/omnitone/#home) - Omnitone: Spatial Audio Rendering on the Web. Omnitone is a robust implementation of [ambisonic](https://en.wikipedia.org/wiki/Ambisonics) decoding and binaural rendering written in Web Audio API. Its rendering process is powered by the fast native features from Web Audio API (GainNode and Convolver), ensuring the optimum performance. The implementation of Omnitone is based on the [Google spatial media](https://github.com/google/spatial-media) specification and [SADIE&#39;s binaural filters](https://www.york.ac.uk/sadie-project/GoogleVRSADIE.html). It also powers [Resonance Audio SDK](https://github.com/resonance-audio/resonance-audio-web-sdk) for web.
* [Mach1 Spatial](https://www.mach1.tech/) - Mach1 Spatial SDK includes APIs to allow developers to design applications that can encode or pan to a spatial audio render from audio streams and/or playback and decode Mach1Spatial 8channel spatial audio mixes with orientation to decode the correct stereo output sum of the user's current orientation. Additionally the Mach1 Spatial SDK allows users to safely convert surround/spatial audio mixes to and from the Mach1Spatial or Mach1Horizon **VVBP** formats.
* [SoundSpaces](https://github.com/facebookresearch/sound-spaces) - SoundSpaces is a realistic acoustic simulation platform for audio-visual embodied AI research. From audio-visual navigation, audio-visual exploration to echolocation and audio-visual floor plan reconstruction, this platform expands embodied vision research to a broader scope of topics.
* [Visual Acoustic Matching](https://github.com/facebookresearch/visual-acoustic-matching) - We introduce the visual acoustic matching task, in which an audio clip is transformed to sound like it was recorded in a target environment. Given an image of the target environment and a waveform for the source audio, the goal is to re-synthesize the audio to match the target room acoustics as suggested by its visible geometry and materials. 

## <span id="wap">Web Audio Processing (WAP)</span>

* [WebRTC Audio Processing](https://github.com/xiongyihui/python-webrtc-audio-processing) - Python binding of WebRTC Audio Processing.
* [MIDI.js](https://galactic.ink/midi-js/) - üéπ Making life easy to create a MIDI-app on the web. Includes a library to program synesthesia into your app for memory recognition or for creating trippy effects. Convert soundfonts for Guitar, Bass, Drums, ect. into code that can be read by the browser. **[MIDI.js](https://github.com/mudcube/MIDI.js)** ties together, and builds upon frameworks that bring MIDI to the browser. Combine it with [jasmid](https://github.com/gasman/jasmid) to create a web-radio MIDI stream similar to this demo, or with [Three.js](https://github.com/mrdoob/three.js/), [Sparks.js](https://github.com/zz85/sparks.js/), or [GLSL](http://glslsandbox.com/) to create Audio/visual experiments.
* [Web Voice Processor](https://github.com/Picovoice/web-voice-processor) - A library for real-time voice processing in web browsers.
* [Tone.js](https://tonejs.github.io/) - Tone.js is a Web Audio framework for creating interactive music in the browser. The architecture of Tone.js aims to be familiar to both musicians and audio programmers creating web-based audio applications. On the high-level, Tone offers common DAW (digital audio workstation) features like a global transport for synchronizing and scheduling events as well as prebuilt synths and effects. Additionally, Tone provides high-performance building blocks to create your own synthesizers, effects, and complex control signals.
* [audio.js](http://kolber.github.io/audiojs/) - audiojs is a drop-in javascript library that allows HTML5's `<audio>` tag to be used anywhere. It uses native `<audio>` where available and falls back to an invisible flash player to emulate it for other browsers. It also serves a consistent html player UI to all browsers which can be styled used standard css.
* [howler.js](https://howlerjs.com/) - Javascript audio library for the modern web. howler.js makes working with audio in JavaScript easy and reliable across all platforms. [howler.js](https://howlerjs.com/) is an audio library for the modern web. It defaults to [Web Audio API](http://webaudio.github.io/web-audio-api/) and falls back to [HTML5 Audio](https://html.spec.whatwg.org/multipage/embedded-content.html#the-audio-element). This makes working with audio in JavaScript easy and reliable across all platforms.
* [CoffeeCollider](https://github.com/mohayonao/CoffeeCollider) - CoffeeCollider is a language for real time audio synthesis and algorithmic composition in HTML5. The concept of this project is designed as "write CoffeeScript, and be processed as SuperCollider."
* [pico.js](http://mohayonao.github.io/pico.js/) - Audio processor for the cross-platform.
* [timbre.js](http://mohayonao.github.io/timbre.js/) - Timbre.js provides a functional processing and synthesizing audio in your web apps with modern JavaScript's way like jQuery or node.js. It has many **T-Object** (formally: Timbre Object) that connected together to define the graph-based routing for overall audio rendering. It is a goal of this project to approach the next generation audio processing for web.
* [Rythm.js](https://okazari.github.io/Rythm.js/) - A javascript library that makes your page dance.
* [p5.sound](https://p5js.org/reference/#/libraries/p5.sound) - p5.sound extends p5 with [Web Audio](http://caniuse.com/audio-api) functionality including audio input, playback, analysis and synthesis.
* [Ableton.js](https://github.com/leolabs/ableton-js) - Ableton.js lets you control your instance or instances of Ableton using Node.js. It tries to cover as many functions as possible.
* [Sound.js](https://github.com/kittykatattack/sound.js) - "Sound.js" is micro-library that lets you load, play, and generate sound effects and music for games and interactive applications. It's very small: less than 800 lines of code and no dependencies. [Click here to try an interactive demo](https://cdn.rawgit.com/kittykatattack/sound.js/master/index.html). You can use it as-as, or integrate it into your existing framework.
* [tuna](https://github.com/Theodeus/tuna) - An audio effects library for the Web Audio API.
* [XSound](https://xsound.jp/) - XSound gives Web Developers Powerful Audio Features Easily !
* [Pizzicato](https://alemangui.github.io/pizzicato/) - A web audio Javascript library. Pizzicato aims to simplify the way you create and manipulate sounds via the Web Audio API. Take a look at the [demo site here](https://alemangui.github.io/pizzicato/). Library to simplify the way you create and manipulate sounds with the Web Audio API.
* [AudioMass](https://github.com/pkalogiros/AudioMass) - Free full-featured web-based audio & waveform editing tool.
* [WebPd](https://github.com/sebpiq/WebPd) - Run your Pure Data patches on the web. WebPd is a compiler for the Pure Data audio programming language allowing to run .pd patches in web pages.
* [DX7 Synth JS](https://github.com/mmontag/dx7-synth-js) - DX7 FM synthesis using the Web Audio and Web MIDI API. Works in Chrome and Firefox. Use a MIDI or QWERTY keyboard to play the synth.
* [WEBMIDI.js](https://github.com/djipco/webmidi) - WEBMIDI.js makes it easy to interact with MIDI instruments directly from a web browser or from Node.js. It simplifies the control of physical or virtual MIDI instruments with user-friendly functions such as playNote(), sendPitchBend() or sendControlChange(). It also allows reacting to inbound MIDI messages by adding listeners for events such as "noteon", "pitchbend" or "programchange".

## <span id="mir">Music Information Retrieval (MIR)</span>

* [Madmom](https://github.com/CPJKU/madmom) - Madmom is an audio signal processing library written in Python with a strong focus on music information retrieval (MIR) tasks.
* [Beets](https://beets.io/) - Beets is the media library management system for obsessive music geeks. music library manager and MusicBrainz tagger.
* [Mido](https://github.com/mido/mido) - MIDI Objects for Python. Mido is a library for working with MIDI messages and ports.
* [mirdata](https://github.com/mir-dataset-loaders/mirdata) - Python library for working with Music Information Retrieval (MIR) datasets.
* [Midifile](https://midifile.sapp.org/) - C++ classes for reading/writing Standard MIDI Files.
* [MSAF](https://msaf.readthedocs.io/en/latest/#) - Music Structure Analysis Framework. A Python framework to analyze music structure. MSAF is a python package for the analysis of music structural segmentation algorithms. It includes a set of features, algorithms, evaluation metrics, and datasets to experiment with.
* [mxml](https://github.com/Reinvent-Archived-Projects/mxml) - MusicXML parsing and layout library. **mxml** is a C++ parser and layout generator for [MusicXML](http://www.musicxml.com/) files.
* [Open-Unmix](https://sigsep.github.io/open-unmix/) - Open-Unmix, Music Source Separation for PyTorch. **Open-Unmix** , is a deep neural network reference implementation for music source separation, applicable for researchers, audio engineers and artists. **Open-Unmix** provides ready-to-use models that allow users to separate pop music into four stems:  **vocals** ,  **drums** , **bass** and the remaining **other** instruments.
* [Spleeter](https://research.deezer.com/projects/spleeter.html) - **Spleeter** is [Deezer](https://www.deezer.com/) source separation library with pretrained models written in [Python](https://www.python.org/) and uses [Tensorflow](https://tensorflow.org/). It makes it easy to train source separation model (assuming you have a dataset of isolated sources), and provides already trained state of the art model for performing various flavour of separation.
* [AMPACT](https://ampact.tumblr.com/) - Automatic Music Performance Analysis and Comparison Toolkit.
* [crema](https://github.com/bmcfee/crema) - convolutional and recurrent estimators for music analysis.
* [MIDIcontroller](https://github.com/joshnishikawa/MIDIcontroller) - A library for creating Teensy MIDI controllers with support for hold or latch buttons, potentiometers, encoders, capacitive sensors, Piezo transducers and other velocity sensitive inputs with aftertouch.
* [MIDI Explorer](https://github.com/EMATech/midiexplorer) - Yet another MIDI monitor, analyzer, debugger and manipulation tool.

## <span id="mg">Music Generation (MG)</span>

* [isobar](https://github.com/ideoforms/isobar) - isobar is a Python library for creating and manipulating musical patterns, designed for use in algorithmic composition, generative music and sonification. It makes it quick and easy to express complex musical ideas, and can send and receive events from various different sources including MIDI, MIDI files, and OSC.
* [MusPy](https://salu133445.github.io/muspy/) - MusPy is an open source Python library for symbolic music generation. It provides essential tools for developing a music generation system, including dataset management, data I/O, data preprocessing and model evaluation.
* [music21](https://web.mit.edu/music21/) - music21 is a Toolkit for Computational Musicology.
* [Msanii](https://kinyugo.github.io/msanii-demo/) - Msanii: High Fidelity Music Synthesis on a Shoestring Budget.
* [MusicLM](https://google-research.github.io/seanet/musiclm/examples/) - MusicLM: Generating Music From Text.
* [SingSong](https://storage.googleapis.com/sing-song/index.html) - SingSong: Generating musical accompaniments from singing.
* [Riffusion App](https://github.com/riffusion/riffusion-app) - Riffusion is an app for real-time music generation with stable diffusion.
* [Mozart](https://github.com/aashrafh/Mozart) - An optical music recognition (OMR) system. Converts sheet music to a machine-readable version. The aim of this project is to develop a sheet music reader. This is called Optical Music Recognition (OMR). Its objective is to convert sheet music to a machine-readable version. We take a simplified version where we convert an image of sheet music to a textual representation that can be further processed to produce midi files or audio files like wav or mp3.
* [Muzic](https://github.com/microsoft/muzic) - Muzic: Music Understanding and Generation with Artificial Intelligence. **Muzic** is a research project on AI music that empowers music understanding and generation with deep learning and artificial intelligence. Muzic is pronounced as [ÀàmjuÀêzeik] and 'Ë∞¨Ë¥ºÂÆ¢' (in Chinese).
* [Jukebox](https://openai.com/blog/jukebox/) - Code for the paper "Jukebox: A Generative Model for Music". We‚Äôre introducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We‚Äôre releasing the model weights and code, along with a tool to explore the generated samples.
* [MidiTok](https://github.com/Natooz/MidiTok) - A convenient MIDI / symbolic music tokenizer for Deep Learning networks, with multiple strategies .üé∂
* [SCAMP](http://scamp.marcevanstein.com/#) - SCAMP is an computer-assisted composition framework in Python designed to act as a hub, flexibly connecting the composer-programmer to a wide variety of resources for playback and notation. SCAMP allows the user to manage the flow of musical time, play notes either using [FluidSynth](http://www.fluidsynth.org/) or via MIDI or OSC messages to an external synthesizer, and ultimately quantize and export the result to music notation in the form of MusicXML or Lilypond. Overall, the framework aims to address pervasive technical challenges while imposing as little as possible on the aesthetic choices of the composer-programmer.
* [Facet](https://github.com/mjcella/facet) - Facet is an open-source live coding system for algorithmic music. With a code editor in the browser and a NodeJS server running locally on your machine, Facet can generate and sequence audio and MIDI data in real-time.Facet is a live coding system for algorithmic music.
* [Mingus](https://github.com/bspaans/python-mingus) - Mingus is a music package for Python. Mingus is a package for Python used by programmers, musicians, composers and researchers to make and analyse music.
* [Audeo](http://faculty.washington.edu/shlizee/audeo/) - ***Audeo*** is a novel system that gets as an input video frames of a musician playing the piano and generates the music for that video. Generation of music from visual cues is a challenging problem and it is not clear whether it is an attainable goal at all. Our main aim in this work is to explore the plausibility of such a transformation and to identify cues and components able to carry the association of sounds with visual events. To achieve the transformation we built a full pipeline named *Audeo* containing three components. We first translate the video frames of the keyboard and the musician hand movements into raw mechanical musical symbolic representation Piano-Roll (Roll) for each video frame which represents the keys pressed at each time step. We then adapt the Roll to be amenable for audio synthesis by including temporal correlations. This step turns out to be critical for meaningful audio generation. As a last step, we implement Midi synthesizers to generate realistic music. *Audeo* converts video to audio smoothly and clearly with only a few setup constraints.
* [libatm](https://github.com/allthemusicllc/libatm) - `libatm` is a library for generating and working with MIDI files. It was purpose-built for All the Music, LLC to assist in its mission to enable musicians to make all of their music without the fear of frivolous copyright lawsuits. All code is released into the public domain via the [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/). If you're looking for a command line tool to generate and work with MIDI files, check out [the `atm-cli` project](https://github.com/allthemusicllc/atm-cli) that utilizes this library. For more information on All the Music, check out [allthemusic.info](http://allthemusic.info/). For more detailed library documentation, check out the crate documentation [here](https://allthemusicllc.github.io/libatm/libatm/index.html).
* [Davidic](https://github.com/bendious/Davidic) - A minimalist procedural music creator. Randomly generate musical scale, MIDI instrument(s), chord progression, and rhythm, then lock-in what you like and regenerate to refine. Advanced controls: chord progressions and rhythms can be manually specified after selecting the Advanced Controls toggle, but UI support is minimal. Suggested usage is restricted to tweaking randomly-generated starting points.
* [PyMusicLooper](https://github.com/arkrow/PyMusicLooper) - A script for creating seamless music loops, with play/export support.
* [ChatGPT2midi](https://github.com/spcoughlin/ChatGPT2midi) - CLI Program for generating chord progressions with ChatGPT.

## <span id="asr">Speech Recognition (ASR)</span>

* [Kaldi](http://kaldi-asr.org/) - Kaldi is a toolkit for speech recognition, intended for use by speech recognition researchers and professionals.
* [PaddleSpeech](https://github.com/PaddlePaddle/PaddleSpeech) - Easy-to-use Speech Toolkit including SOTA/Streaming ASR with punctuation, influential TTS with text frontend, Speaker Verification System, End-to-End Speech Translation and Keyword Spotting.
* [NVIDIA NeMo](https://nvidia.github.io/NeMo/) - NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), natural language processing (NLP), and text-to-speech synthesis (TTS). The primary objective of NeMo is to help researchers from industry and academia to reuse prior work (code and pretrained models) and make it easier to create new [conversational AI models](https://developer.nvidia.com/conversational-ai#started).
* [Whisper](https://github.com/openai/whisper) - Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.
* [Transformers](https://huggingface.co/docs/transformers/index) - ü§ó Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.
* [Julius](https://github.com/julius-speech/julius) - Open-Source Large Vocabulary Continuous Speech Recognition Engine. "Julius" is a high-performance, small-footprint large vocabulary continuous speech recognition (LVCSR) decoder software for speech-related researchers and developers. The main platform is Linux and other Unix-based system, as well as Windows, Mac, Androids and other platforms.
* [audino](https://github.com/midas-research/audino) - audino is an open source audio annotation tool. It provides features such as transcription and labeling which enables annotation for Voice Activity Detection (VAD), Diarization, Speaker Identification, Automated Speech Recognition, Emotion Recognition tasks and more.
* [Wenet](https://wenet.org.cn/wenet/) - Wenet is an tansformer-based end-to-end ASR toolkit.
* [SpeechBrain](https://speechbrain.github.io/) - SpeechBrain is an **open-source** and **all-in-one** conversational AI toolkit based on PyTorch. The goal is to create a  **single** ,  **flexible** , and **user-friendly** toolkit that can be used to easily develop  **state-of-the-art speech technologies** , including systems for  **speech recognition** ,  **speaker recognition** ,  **speech enhancement** ,  **speech separation** ,  **language identification** ,  **multi-microphone signal processing** , and many others.
* [ESPnet](https://espnet.github.io/espnet/) - ESPnet is an end-to-end speech processing toolkit, mainly focuses on end-to-end speech recognition and end-to-end text-to-speech. ESPnet is an end-to-end speech processing toolkit covering end-to-end speech recognition, text-to-speech, speech translation, speech enhancement, speaker diarization, spoken language understanding, and so on. ESPnet uses [pytorch](http://pytorch.org/) as a deep learning engine and also follows [Kaldi](http://kaldi-asr.org/) style data processing, feature extraction/format, and recipes to provide a complete setup for various speech processing experiments.
* [Espresso](https://github.com/freewym/espresso) - Espresso is an open-source, modular, extensible end-to-end neural automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch and the popular neural machine translation toolkit fairseq. 
* [Leon](https://getleon.ai/) - üß† Leon is your open-source personal assistant.
* [DeepSpeech](https://github.com/mozilla/DeepSpeech) - DeepSpeech is an open source embedded (offline, on-device) speech-to-text engine which can run in real time on devices ranging from a Raspberry Pi 4 to high power GPU servers.
* [annyang](https://www.talater.com/annyang/) - annyang is a tiny javascript library that lets your visitors control your site with voice commands. annyang supports multiple languages, has no dependencies, weighs just 2kb and is free to use.
* [PocketSphinx](https://github.com/cmusphinx/pocketsphinx) - This is PocketSphinx, one of Carnegie Mellon University's open source large vocabulary, speaker-independent continuous speech recognition engines.
* [Kara](https://github.com/emileclarkb/Kara) - Open Source Voice Assistant. Simply put, Kara is a voice assistant that steals 0% of your data so you stay free! She is a actively maintained, modular, and designed to customize.
* [Voice Lab](https://github.com/Voice-Lab/VoiceLab) - Voice Lab is an automated voice analysis software. What this software does is allow you to measure, manipulate, and visualize many voices at once, without messing with analysis parameters. You can also save all of your data, analysis parameters, manipulated voices, and full colour spectrograms and power spectra, with the press of one button.

## <span id="tts">Speech Synthesis (TTS)</span>

* [VALL-E](https://valle-demo.github.io/) - VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers.
* [VITS](https://github.com/jaywalnut310/vits) - VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech. Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text.
* [NeuralSpeech](https://github.com/microsoft/NeuralSpeech) - **NeuralSpeech** is a research project in Microsoft Research Asia focusing on neural network based speech processing, including automatic speech recognition (ASR), text to speech (TTS), etc.
* [Real-Time Voice Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) - Clone a voice in 5 seconds to generate arbitrary speech in real-time.  This repository is an implementation of [Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis](https://arxiv.org/pdf/1806.04558.pdf) (SV2TTS) with a vocoder that works in real-time. SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.
* [WaveNet](https://github.com/ibab/tensorflow-wavenet) - A TensorFlow implementation of DeepMind's WaveNet paper. The WaveNet neural network architecture directly generates a raw audio waveform, showing excellent results in text-to-speech and general audio generation (see the DeepMind blog post and paper for details).
* [FastSpeech 2](https://github.com/ming024/FastSpeech2) - An implementation of Microsoft's "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech".
* [MelGAN](http://swpark.me/melgan/) - Generative Adversarial Networks for Conditional Waveform Synthesis.
* [HiFi-GAN](https://github.com/jik876/hifi-gan) - HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis.
* [edge-tts](https://github.com/rany2/edge-tts) - Use Microsoft Edge's online text-to-speech service from Python (without needing Microsoft Edge/Windows or an API key).
* [Vocode](https://docs.vocode.dev/) - Vocode is an open-source library for building voice-based LLM applications.
* [TTS-dataset-tools](https://github.com/youmebangbang/TTS-dataset-tools) - Automatically generates TTS dataset using audio and associated text. Make cuts under a custom length. Uses Google Speech to text API to perform diarization and transcription or aeneas to force align text to audio.

## <span id="svs">Singing Voice Synthesis (SVS)</span>

* [NNSVS](https://nnsvs.github.io/) - Neural network-based singing voice synthesis library for research.
* [Muskit](https://github.com/SJTMusicTeam/Muskits) - Muskit is an open-source music processing toolkit. Currently we mostly focus on benchmarking the end-to-end singing voice synthesis and expect to extend more tasks in the future. Muskit employs [pytorch](http://pytorch.org/) as a deep learning engine and also follows [ESPnet](https://github.com/espnet/espnet) and [Kaldi](http://kaldi-asr.org/) style data processing, and recipes to provide a complete setup for various music processing experiments.
* [OpenUtau](http://www.openutau.com/) - Open singing synthesis platform / Open source UTAU successor.
* [so-vits-svc](https://github.com/svc-develop-team/so-vits-svc) - SoftVC VITS Singing Voice Conversion.
* [Retrieval-based-Voice-Conversion-WebUI](https://github.com/liujing04/Retrieval-based-Voice-Conversion-WebUI) - An easy-to-use SVC framework based on VITS.
* [Sinsy](http://sinsy.jp/) - Sinsy is an HMM/DNN-based singing voice synthesis system. You can generate a singing voice sample by uploading the musical score (MusicXML) to this website.
* [DiffSinger](https://github.com/MoonInTheRiver/DiffSinger) - DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism.
* [lessampler](https://www.gloomyghost.com/lessampler/#/) - lessampler is a Singing Voice Synthesizer. It provides complete pitch shifting, time stretching and other functions. Support multiple interface calls such as UTAU, Library, and Shine.
* [Mellotron](https://github.com/NVIDIA/mellotron) - Mellotron: a multispeaker voice synthesis model based on Tacotron 2 GST that can make a voice emote and sing without emotive or singing training data.

## And more
